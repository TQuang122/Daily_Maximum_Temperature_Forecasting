# Southern Daily Maximum Temperature Forecasting: Dá»± bÃ¡o Nhiá»‡t Äá»™ Tá»‘i Äa Nam Bá»™ Viá»‡t Nam

## Tá»•ng Quan

**Daily Maximum Temperature Forecasting** lÃ  má»™t dá»± Ã¡n Machine Learning dá»± bÃ¡o nhiá»‡t Ä‘á»™ tá»‘i Ä‘a hÃ ng ngÃ y cho 18 tá»‰nh/thÃ nh phá»‘ khu vá»±c Nam Bá»™ Viá»‡t Nam. Dá»± Ã¡n sá»­ dá»¥ng dá»¯ liá»‡u thá»i tiáº¿t lá»‹ch sá»­ tá»« 2015-2025 vÃ  Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n ensemble learning tiÃªn tiáº¿n Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao.

### Má»¥c TiÃªu
- Dá»± bÃ¡o nhiá»‡t Ä‘á»™ tá»‘i Ä‘a ngÃ y mai vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao (MAE < 1.0Â°C)
- PhÃ¢n tÃ­ch cÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng Ä‘áº¿n nhiá»‡t Ä‘á»™
- So sÃ¡nh hiá»‡u quáº£ cá»§a cÃ¡c thuáº­t toÃ¡n ML khÃ¡c nhau
- Tá»‘i Æ°u hÃ³a hyperparameters Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t

### Káº¿t Quáº£ ChÃ­nh
- **MÃ´ hÃ¬nh tá»‘t nháº¥t**: Random Forest (tá»‘i Æ°u hÃ³a vá»›i Optuna)
- **Test MAE**: 0.9934Â°C
- **Test RMSE**: 1.2854Â°C  
- **Test RÂ²**: 0.6056
- **Sá»‘ lÆ°á»£ng features**: 23 Ä‘áº·c trÆ°ng Ä‘Æ°á»£c chá»n lá»c
- **Dá»¯ liá»‡u training**: 62,208 máº«u (2015-2024)
- **Dá»¯ liá»‡u test**: 6,444 máº«u (2024-2025)

## ğŸ—‚ï¸ Cáº¥u TrÃºc Dá»± Ãn

```
DeepThermo/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ raw/                                    # Dá»¯ liá»‡u gá»‘c
â”‚   â”‚   â””â”€â”€ Southern_Vietnam_Weather_2015-2025.csv
â”‚   â””â”€â”€ processed/                              # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”‚       â”œâ”€â”€ Southern_Vietnam_Weather_processed.csv
â”‚       â””â”€â”€ splits/                             # Chia train/val/test
â”‚           â”œâ”€â”€ raw_train.csv, raw_val.csv, raw_test.csv
â”‚           â”œâ”€â”€ fe_train.csv, fe_val.csv, fe_test.csv
â”‚           â”œâ”€â”€ dt_train.csv, dt_val.csv, dt_test.csv
â”‚           â””â”€â”€ fe_dt_train.csv, fe_dt_val.csv, fe_dt_test.csv
â”œâ”€â”€ notebooks/                                  # Jupyter notebooks
â”‚   â”œâ”€â”€ 1_Preprocessing.ipynb                  # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 2_FeatureEngineering.ipynb             # Ká»¹ thuáº­t Ä‘áº·c trÆ°ng
â”‚   â”œâ”€â”€ 3_Modelling.ipynb                      # XÃ¢y dá»±ng mÃ´ hÃ¬nh
â”‚   â””â”€â”€ 4_Hyperparameter_Optimization.ipynb    # Tá»‘i Æ°u hyperparameters
â”œâ”€â”€ src/
â”‚   â””â”€â”€ utils/                                 # Utilities
â”‚       â”œâ”€â”€ scores.py                          # HÃ m Ä‘Ã¡nh giÃ¡ metrics
â”‚       â”œâ”€â”€ visualization.py                   # HÃ m váº½ biá»ƒu Ä‘á»“
â”‚       â””â”€â”€ performance_monitor.py             # GiÃ¡m sÃ¡t hiá»‡u suáº¥t
â”œâ”€â”€ figures/                                    # Biá»ƒu Ä‘á»“ vÃ  hÃ¬nh áº£nh
â”œâ”€â”€ models/                                     # MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u
â”œâ”€â”€ main.py                                     # Entry point
â”œâ”€â”€ requirements.txt                            # Dependencies
â””â”€â”€ pyproject.toml                             # Project configuration
```

## CÃ i Äáº·t

### YÃªu Cáº§u Há»‡ Thá»‘ng
- Python >= 3.12.11
- RAM >= 8GB (khuyáº¿n nghá»‹ 16GB)
- Disk space >= 2GB

### CÃ i Äáº·t Dependencies

```bash
# Clone repository
git clone <repository-url>
cd DeepThermo

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Hoáº·c sá»­ dá»¥ng uv (khuyáº¿n nghá»‹)
uv sync
```

### Dependencies ChÃ­nh
- **Core ML**: scikit-learn, xgboost, lightgbm, random forest, adaboost
- **Optimization**: optuna
- **Visualization**: matplotlib, seaborn
- **Data Processing**: pandas, numpy
- **Time Series**: statsmodels, prophet
- **Interpretation**: shap

## ğŸ“ˆ Quy TrÃ¬nh PhÃ¡t Triá»ƒn

### 1. Tiá»n Xá»­ LÃ½ Dá»¯ Liá»‡u (`1_Preprocessing.ipynb`)

**Má»¥c tiÃªu**: LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u thá»i tiáº¿t

**CÃ¡c bÆ°á»›c chÃ­nh**:
- **Kiá»ƒm tra dá»¯ liá»‡u**: 70,146 dÃ²ng, 33 cá»™t, 18 tá»‰nh/thÃ nh phá»‘
- **Xá»­ lÃ½ missing values**: 3 cá»™t cÃ³ missing (severerisk: 65.85%, preciptype: 22.72%, visibility: 11.28%)
- **Chuyá»ƒn Ä‘á»•i Ä‘Æ¡n vá»‹**: Â°F â†’ Â°C cho táº¥t cáº£ biáº¿n nhiá»‡t Ä‘á»™
- **Loáº¡i bá» cá»™t khÃ´ng cáº§n thiáº¿t**: 19 cá»™t Ä‘Æ°á»£c loáº¡i bá»
- **PhÃ¡t hiá»‡n outliers**: Sá»­ dá»¥ng IQR vÃ  Z-score methods
- **ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng dá»¯ liá»‡u**: Completeness: 100%, Validity: 100%

**Káº¿t quáº£**: Dataset sáº¡ch vá»›i 14 features quan trá»ng

### 2. Ká»¹ Thuáº­t Äáº·c TrÆ°ng (`2_FeatureEngineering.ipynb`)

**Má»¥c tiÃªu**: Táº¡o Ä‘áº·c trÆ°ng má»›i vÃ  chá»n lá»c features hiá»‡u quáº£

**CÃ¡c ká»¹ thuáº­t Ã¡p dá»¥ng**:

#### 2.1. Táº¡o Target Variable
- **Target**: `tempmax_C` cá»§a ngÃ y mai (shift -1)
- **Loáº¡i bá» leakage**: Sá»­ dá»¥ng groupby theo tá»‰nh Ä‘á»ƒ trÃ¡nh data leakage

#### 2.2. Chia Dá»¯ Liá»‡u Thá»i Gian
- **Train**: 2015-01-01 â†’ 2024-01-01 (9 nÄƒm)
- **Validation**: 2024-02-01 â†’ 2024-07-31 (6 thÃ¡ng)
- **Test**: 2024-08-31 â†’ 2025-08-30 (12 thÃ¡ng)
- **Gap**: 30 ngÃ y giá»¯a cÃ¡c táº­p Ä‘á»ƒ trÃ¡nh data leakage

#### 2.3. Feature Engineering
- **Lag features**: `tempmax_C_lag1`, `tempmax_C_lag2`, `tempmax_C_lag3`, `tempmax_C_lag4`
- **Rolling statistics**: 
  - `tempmax_C_rollmean7` (7 ngÃ y)
  - `tempmax_C_rollmean3` (3 ngÃ y)
  - `tempmax_C_rollstd7` (7 ngÃ y)
- **Seasonal features**:
  - `month_sin`, `month_cos` (chu ká»³ thÃ¡ng)
  - `doy_sin`, `doy_cos` (chu ká»³ nÄƒm)
- **Temporal features**: `year`, `day_of_year`, `day_of_week`

#### 2.4. Feature Selection
- **Mutual Information**: Chá»n top features cÃ³ MI cao nháº¥t
- **Decision Tree**: Sá»­ dá»¥ng feature importance Ä‘á»ƒ chá»n lá»c
- **Káº¿t quáº£**: 23 features cuá»‘i cÃ¹ng Ä‘Æ°á»£c chá»n

### 3. XÃ¢y Dá»±ng MÃ´ HÃ¬nh (`3_Modelling.ipynb`)

**Má»¥c tiÃªu**: So sÃ¡nh hiá»‡u quáº£ cá»§a cÃ¡c thuáº­t toÃ¡n ML khÃ¡c nhau

**CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡**:
1. **Random Forest**
2. **AdaBoost** 
3. **Gradient Boosting**
4. **XGBoost**

**PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡**:
- **Cross-validation**: TimeSeriesSplit vá»›i 6-fold, gap=7
- **Metrics**: MAE, RMSE, RÂ²
- **4 datasets**: Original, FE, Original+DT, FE+DT

**Káº¿t quáº£ so sÃ¡nh**:

| Model | Dataset | Val MAE | Test MAE | Test RMSE | Test RÂ² |
|-------|---------|---------|----------|-----------|---------|
| Random Forest | FE+DT | 0.9394 | 0.9903 | 1.2818 | 0.6078 |
| XGBoost | FE+DT | 0.9830 | 1.0457 | 1.3546 | 0.5620 |
| Gradient Boosting | FE+DT | 0.9308 | 1.0135 | 1.3310 | 0.5771 |
| AdaBoost | FE+DT | 1.0824 | 1.0879 | 1.3878 | 0.5403 |

**MÃ´ hÃ¬nh tá»‘t nháº¥t**: Random Forest trÃªn dataset FE+DT

### 4. Tá»‘i Æ¯u Hyperparameters (`4_Hyperparameter_Optimization.ipynb`)

**Má»¥c tiÃªu**: Tá»‘i Æ°u hÃ³a hyperparameters Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t

**PhÆ°Æ¡ng phÃ¡p**: Optuna vá»›i TPE Sampler vÃ  Median Pruner

**CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tá»‘i Æ°u**:
1. **XGBoost**: 100 trials
2. **Random Forest**: 50 trials  
3. **Gradient Boosting**: 50 trials
4. **LightGBM**: 80 trials

**Káº¿t quáº£ tá»‘i Æ°u**:

| Model | CV MAE | Test MAE | Test RMSE | Test RÂ² |
|-------|--------|----------|-----------|---------|
| Random Forest | 0.5452 | **0.9934** | 1.2854 | 0.6056 |
| XGBoost | 0.5137 | 1.0000 | 1.3047 | 0.5937 |
| Gradient Boosting | 0.5131 | 1.0135 | 1.3310 | 0.5771 |
| LightGBM | 0.5551 | 1.0410 | 1.3522 | 0.5636 |

**MÃ´ hÃ¬nh cuá»‘i cÃ¹ng**: Random Forest vá»›i hyperparameters tá»‘i Æ°u

## ğŸ”§ Sá»­ Dá»¥ng

### Cháº¡y Notebooks

```bash
# Khá»Ÿi Ä‘á»™ng Jupyter
jupyter notebook

# Hoáº·c sá»­ dá»¥ng JupyterLab
jupyter lab
```

### Cháº¡y Main Script

```bash
python main.py
```

### Load MÃ´ HÃ¬nh ÄÃ£ LÆ°u

```python
import joblib
import json

# Load model
model = joblib.load('models/rf_optimized_20251027_173819.joblib')

# Load metadata
with open('models/rf_optimized_metadata_20251027_173819.json', 'r') as f:
    metadata = json.load(f)

# Dá»± bÃ¡o
predictions = model.predict(X_test)
```

## ğŸ“Š PhÃ¢n TÃ­ch Káº¿t Quáº£

### Feature Importance (Top 10)

| Feature | Importance | MÃ´ táº£ |
|---------|------------|-------|
| tempmax_C_rollmean3 | 0.226 | Nhiá»‡t Ä‘á»™ TB 3 ngÃ y gáº§n nháº¥t |
| temp_C | 0.199 | Nhiá»‡t Ä‘á»™ trung bÃ¬nh |
| tempmax_C_rollmean7 | 0.104 | Nhiá»‡t Ä‘á»™ TB 7 ngÃ y gáº§n nháº¥t |
| tempmax_C_lag1 | 0.057 | Nhiá»‡t Ä‘á»™ tá»‘i Ä‘a ngÃ y hÃ´m qua |
| feelslike_C | 0.037 | Nhiá»‡t Ä‘á»™ cáº£m nháº­n |
| feelslikemax_C | 0.035 | Nhiá»‡t Ä‘á»™ cáº£m nháº­n tá»‘i Ä‘a |
| solarradiation | 0.034 | Bá»©c xáº¡ máº·t trá»i |
| winddir | 0.029 | HÆ°á»›ng giÃ³ |
| humidity | 0.027 | Äá»™ áº©m |
| sealevelpressure | 0.027 | Ãp suáº¥t má»±c nÆ°á»›c biá»ƒn |

### Insights ChÃ­nh

1. **Nhiá»‡t Ä‘á»™ lá»‹ch sá»­** lÃ  yáº¿u tá»‘ quan trá»ng nháº¥t (lag features + rolling means)
2. **Nhiá»‡t Ä‘á»™ hiá»‡n táº¡i** cÃ³ tÃ¡c Ä‘á»™ng lá»›n Ä‘áº¿n dá»± bÃ¡o
3. **Bá»©c xáº¡ máº·t trá»i** áº£nh hÆ°á»Ÿng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n nhiá»‡t Ä‘á»™
4. **Äá»™ áº©m** cÃ³ tÆ°Æ¡ng quan Ã¢m vá»›i nhiá»‡t Ä‘á»™ (cÃ ng áº©m cÃ ng mÃ¡t)
5. **Ãp suáº¥t khÃ­ quyá»ƒn** cÅ©ng lÃ  yáº¿u tá»‘ quan trá»ng

## ğŸ“ˆ Biá»ƒu Äá»“ vÃ  HÃ¬nh áº¢nh

Dá»± Ã¡n bao gá»“m 22 biá»ƒu Ä‘á»“ phÃ¢n tÃ­ch:

### PhÃ¢n TÃ­ch Dá»¯ Liá»‡u
- `temperature_distribution.png`: PhÃ¢n phá»‘i nhiá»‡t Ä‘á»™
- `humidity_distribution.png`: PhÃ¢n phá»‘i Ä‘á»™ áº©m
- `correlation_heatmap_numeric_features.png`: Ma tráº­n tÆ°Æ¡ng quan
- `time_series_analysis.png`: PhÃ¢n tÃ­ch chuá»—i thá»i gian
- `seasonal_decomposition.png`: PhÃ¢n tÃ­ch mÃ¹a vá»¥

### So SÃ¡nh MÃ´ HÃ¬nh
- `model_compare_Val_MAE.png`: So sÃ¡nh MAE validation
- `model_compare_Test_MAE.png`: So sÃ¡nh MAE test
- `model_compare_Test_R2.png`: So sÃ¡nh RÂ² test

### PhÃ¢n TÃ­ch Äá»‹a LÃ½
- `average_max_temp_by_province.png`: Nhiá»‡t Ä‘á»™ TB theo tá»‰nh
- `monthly_average_max_temp_by_province.png`: Nhiá»‡t Ä‘á»™ theo thÃ¡ng
- `heatmap_average_max_temp_by_year_month.png`: Heatmap nhiá»‡t Ä‘á»™

## âš¡ Hiá»‡u Suáº¥t

### Thá»i Gian Thá»±c Thi
- **Data Loading**: ~0.1s
- **Feature Engineering**: ~2-3 phÃºt
- **Model Training**: ~2-10 phÃºt (tÃ¹y mÃ´ hÃ¬nh)
- **Hyperparameter Optimization**: ~1-6 giá»
- **Prediction**: <0.1s

### Sá»­ Dá»¥ng Bá»™ Nhá»›
- **Peak Memory**: ~1.2GB
- **Model Size**: ~50MB (Random Forest)
- **Dataset Size**: ~17MB (processed)

## ğŸ”¬ PhÆ°Æ¡ng PhÃ¡p Khoa Há»c

### 1. Xá»­ LÃ½ Data Leakage
- Sá»­ dá»¥ng `groupby` theo tá»‰nh Ä‘á»ƒ táº¡o lag features
- Chia dá»¯ liá»‡u theo thá»i gian vá»›i gap 30 ngÃ y
- TimeSeriesSplit cho cross-validation

### 2. Feature Engineering
- **Temporal features**: Lag, rolling statistics, seasonal encoding
- **Domain knowledge**: CÃ¡c biáº¿n khÃ­ tÆ°á»£ng quan trá»ng
- **Feature selection**: Mutual Information + Decision Tree

### 3. Model Selection
- **Ensemble methods**: Random Forest, XGBoost, Gradient Boosting
- **Hyperparameter optimization**: Optuna vá»›i TPE sampler
- **Cross-validation**: TimeSeriesSplit Ä‘á»ƒ trÃ¡nh overfitting

### 4. Evaluation
- **Metrics**: MAE (chÃ­nh), RMSE, RÂ²
- **Time series validation**: KhÃ´ng shuffle dá»¯ liá»‡u
- **Hold-out test**: 12 thÃ¡ng cuá»‘i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng

## ğŸš€ Cáº£i Tiáº¿n Trong TÆ°Æ¡ng Lai

### 1. MÃ´ HÃ¬nh NÃ¢ng Cao
- **Deep Learning**: LSTM, GRU, Transformer
- **Ensemble**: Stacking, Blending
- **AutoML**: Auto-sklearn, TPOT

### 2. Dá»¯ Liá»‡u Bá»• Sung
- **Satellite data**: MODIS, Landsat
- **Reanalysis data**: ERA5, NCEP
- **Social media**: Twitter sentiment vá» thá»i tiáº¿t

### 3. TÃ­nh NÄƒng Má»›i
- **Uncertainty quantification**: Bayesian methods
- **Real-time prediction**: API service
- **Mobile app**: á»¨ng dá»¥ng di Ä‘á»™ng

### 4. Má»Ÿ Rá»™ng Pháº¡m Vi
- **ToÃ n quá»‘c**: 63 tá»‰nh/thÃ nh phá»‘
- **Äá»™ phÃ¢n giáº£i cao**: Cáº¥p huyá»‡n/xÃ£
- **Dá»± bÃ¡o dÃ i háº¡n**: 7-30 ngÃ y

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

### Papers
1. "Weather Forecasting using Machine Learning" - Journal of Applied Meteorology
2. "Time Series Forecasting with XGBoost" - ICML 2023
3. "Feature Engineering for Time Series" - KDD 2022

### Datasets
- [Weather API](https://www.visualcrossing.com/)
- [NOAA Climate Data](https://www.ncdc.noaa.gov/)
- [Vietnam Meteorological Data](http://www.imh.ac.vn/)

### Tools
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Scikit-learn Documentation](https://scikit-learn.org/)

## ğŸ‘¥ ÄÃ³ng GÃ³p




